{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Requirement Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, json\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from openai import OpenAI\n",
    "from urllib.parse import urlencode\n",
    "import re\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "import concurrent.futures\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import csv\n",
    "from urllib.parse import urljoin, urlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': ''\n",
    "}\n",
    "# OpenAI API\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "# First, we create a EventHandler class to define\n",
    "# how we want to handle the events in the response stream.\n",
    "class EventHandler(AssistantEventHandler):    \n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\noutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV File and Get Company Name and Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(csv_file, start, end):\n",
    "    csv_data = {}\n",
    "    try:\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for idx, row in enumerate(reader):\n",
    "                if idx < start:\n",
    "                    continue\n",
    "                if end is not None and idx > end:\n",
    "                    break\n",
    "                company_name = row['Company name']\n",
    "                websites = row['Website']\n",
    "                csv_data[company_name] = websites\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata_buyer51_100 = \"testingdata_buyer51_100.csv\"\n",
    "start = 44\n",
    "end = 51\n",
    "csv_data_website = read_csv_file(testingdata_buyer51_100, start, end)\n",
    "csv_data_website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Website Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sitemap Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sitemap_url is the function to get the base sitemap URL\n",
    "def get_sitemap_url(base_url):\n",
    "    def fetch_sitemap(scrapeops_api_key, url):\n",
    "        # Proxy settings\n",
    "        proxy_params = {\n",
    "            'api_key': scrapeops_api_key,\n",
    "            'url': url,\n",
    "            'render_js': True,\n",
    "        }\n",
    "    \n",
    "        # Send a GET request to the proxy URL\n",
    "        response = requests.get(url='https://proxy.scrapeops.io/v1/',\n",
    "                                params=urlencode(proxy_params),\n",
    "                                headers={'User-Agent': ''})\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        return None\n",
    "\n",
    "    scrapeops_api_key = \"\"  # API Key\n",
    "    # Normalize the base URL\n",
    "    base_url = base_url.rstrip('/')\n",
    "    # Try common sitemap URLs\n",
    "    common_paths = []\n",
    "    for path in common_paths:\n",
    "        url = urljoin(base_url, path)\n",
    "        sitemap_content = fetch_sitemap(scrapeops_api_key, url)\n",
    "        if sitemap_content:\n",
    "            print('Sitemap URL found:', url)\n",
    "            return url, sitemap_content\n",
    "    \n",
    "    # Try parsing robots.txt for sitemap entries\n",
    "    robots_url = urljoin(base_url, 'robots.txt')\n",
    "    robots_content = fetch_sitemap(scrapeops_api_key, robots_url)\n",
    "    if robots_content:\n",
    "        # Lines that contain 'Sitemap:' directive\n",
    "        lines = robots_content.splitlines()\n",
    "        for line in lines:\n",
    "            if line.strip().lower().startswith('sitemap:'):\n",
    "                sitemap_path = line.split(':', 1)[1].strip()\n",
    "                if sitemap_path:\n",
    "                    url = urljoin(base_url, sitemap_path)\n",
    "                    sitemap_content = fetch_sitemap(scrapeops_api_key, url)\n",
    "                    if sitemap_content:\n",
    "                        print('Sitemap URL found via robots.txt:', url)\n",
    "                        return url, sitemap_content\n",
    "\n",
    "    print('No sitemap found')\n",
    "    return None, None\n",
    "\n",
    "# find_additional_sitemaps is the function to find additional sitemaps under base sitemap\n",
    "def find_additional_sitemaps(sitemap_content):\n",
    "    additional_sitemaps = []\n",
    "    try:\n",
    "        root = ET.fromstring(sitemap_content)\n",
    "        namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        for sitemap in root.findall('sitemap:sitemap', namespace):\n",
    "            loc = sitemap.find('sitemap:loc', namespace)\n",
    "            if loc is not None:\n",
    "                sitemap_url = loc.text\n",
    "                additional_sitemaps.append(sitemap_url)\n",
    "    except ET.ParseError:\n",
    "        print('Error parsing the sitemap content')\n",
    "        \n",
    "    return additional_sitemaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url is website base URL\n",
    "base_url = ''\n",
    "\n",
    "sitemap_url, sitemap_content = get_sitemap_url(base_url)\n",
    "print(sitemap_url)\n",
    "additional_sitemaps = find_additional_sitemaps(sitemap_content)\n",
    "print(additional_sitemaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Product URLs from Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_product_urls is the function that extract product URLs from the sitemap\n",
    "def extract_product_urls(sitemap_url, scrapeops_api_key, pattern, max_urls):\n",
    "    # Proxy settings\n",
    "    proxy_params = {\n",
    "        'api_key': scrapeops_api_key,\n",
    "        'url': sitemap_url,\n",
    "        'render_js': True,\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the proxy URL\n",
    "    response = requests.get(url='https://proxy.scrapeops.io/v1/',\n",
    "                            params=urlencode(proxy_params),\n",
    "                            headers={'User-Agent': ''})\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML content\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Define namespace\n",
    "        namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        # Extract URLs that contain the specific pattern in their path\n",
    "        product_urls = [\n",
    "            url.find('sitemap:loc', namespace).text \n",
    "            for url in root.findall('sitemap:url', namespace) \n",
    "            if pattern in url.find('sitemap:loc', namespace).text\n",
    "        ]\n",
    "        \n",
    "        # Check if the number of product URLs exceeds the maximum allowed\n",
    "        print(len(product_urls))\n",
    "        if len(product_urls) > max_urls:\n",
    "            print(\"!!!WARNING!!! There are too many links in the list and might take a long time to process. Should consider separating into smaller lists.\")\n",
    "        else:\n",
    "            print(f\"There are {len(product_urls)} links in the scraping list.\")\n",
    "        return product_urls\n",
    "    \n",
    "    # Failed to retrieve the sitemap\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the sitemap, status code: {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information input for the function\n",
    "sitemap_url = \"\"  # Sitemap Link\n",
    "company_name = \"\"  # Provide Company Name\n",
    "scrapeops_api_key = \"\"  # API Key\n",
    "pattern = ''  # URL pattern\n",
    "max_urls= 500  # Maximum number of URLs\n",
    "\n",
    "# Call extract_product_urls function to extract product URLs\n",
    "product_urls = []\n",
    "product_urls = extract_product_urls(sitemap_url, scrapeops_api_key, pattern, max_urls)\n",
    "# print(product_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Product Information from Product URLs - Single Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect_product_pages_single is the function that collects product pages from the list of product URLs\n",
    "def collect_product_pages_single(product_urls, scrapeops_api_key, headers, client, is_seller):\n",
    "    results = []\n",
    "    final_tokens = []\n",
    "    \n",
    "    for link in product_urls:\n",
    "        proxy_params = {\n",
    "            'api_key': scrapeops_api_key,\n",
    "            'url': link,\n",
    "            'render_js': True,\n",
    "        }\n",
    "        # Send a GET request to the URL with the headers\n",
    "        response = requests.get(url='https://proxy.scrapeops.io/v1/',\n",
    "                                params=urlencode(proxy_params),\n",
    "                                headers=headers)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            prod = str(soup.get_text(separator=' ', strip=True))\n",
    "            \n",
    "            if len(prod) > 256000:\n",
    "                results.append({\"product_name\": \"Pass\"})\n",
    "                continue\n",
    "            else:\n",
    "                print(link)\n",
    "                thread = client.beta.threads.create()\n",
    "                message = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    role=\"user\",\n",
    "                    content=f\"Please follow the instruction to check if the following content is product information or not and summarize the information into a JSON format with attributes (product_name, product_detail, and product_application) if it is a product information page: {prod}\"\n",
    "                )\n",
    "                # Then, we use the `stream` SDK helper with the `EventHandler` class to create the Run and stream the response.\n",
    "                if is_seller:\n",
    "                    with client.beta.threads.runs.stream(\n",
    "                            thread_id=thread.id,\n",
    "                            assistant_id=\"\",\n",
    "                            event_handler=EventHandler(),\n",
    "                    ) as stream:\n",
    "                        stream.until_done()\n",
    "                else:\n",
    "                    with client.beta.threads.runs.stream(\n",
    "                            thread_id=thread.id,\n",
    "                            assistant_id=\"\",\n",
    "                            event_handler=EventHandler(),\n",
    "                    ) as stream:\n",
    "                        stream.until_done()\n",
    "                # Retrieve the final message\n",
    "                messages = client.beta.threads.messages.list(\n",
    "                    thread_id=thread.id\n",
    "                )\n",
    "                final_message = messages.data[0].content[0].text.value \n",
    "                results.append(final_message)\n",
    "                # Retrieve the final tokens\n",
    "                runs = client.beta.threads.runs.list(\n",
    "                    thread_id=thread.id\n",
    "                )\n",
    "                final_tokens.append({\"completion_tokens\": runs.data[0].usage.completion_tokens, \n",
    "                \"prompt_tokens\": runs.data[0].usage.prompt_tokens, \n",
    "                \"total_tokens\": runs.data[0].usage.total_tokens})\n",
    "        # Failed to retrieve the webpage\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage {link}, status code: {response.status_code}\")\n",
    "            results.append(str({\"product_name\": \"Pass\"}))\n",
    "\n",
    "    return results, final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if the company is a seller, False if the company is a buyer\n",
    "is_seller = False\n",
    "\n",
    "# Call collect_product_pages_single function to collect product pages\n",
    "results, final_tokens = collect_product_pages_single(product_urls, scrapeops_api_key, headers, client, is_seller)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Product Information from Product URLs -  Multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_url is function to handle if the product URL request return status code is 429\n",
    "def process_url(link, queue, scrapeops_api_key, headers, client, is_seller, final_tokens):\n",
    "    proxy_params = {\n",
    "        'api_key': scrapeops_api_key,\n",
    "        'url': link,\n",
    "        'render_js': True,\n",
    "    }\n",
    "    headers = headers\n",
    "\n",
    "    retries = 2\n",
    "    backoff_factor = 5\n",
    "\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url='https://proxy.scrapeops.io/v1/',\n",
    "                                    params=urlencode(proxy_params),\n",
    "                                    headers=headers,\n",
    "                                    timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                prod = str(soup.get_text(separator=' ', strip=True))\n",
    "                if len(prod) > 256000:\n",
    "                    queue.put(json.dumps({\"product_name\": \"Pass\"}))\n",
    "                    return\n",
    "                else:\n",
    "                    print(link)\n",
    "                    thread = client.beta.threads.create()\n",
    "                    message = client.beta.threads.messages.create(\n",
    "                        thread_id=thread.id,\n",
    "                        role=\"user\",\n",
    "                        content=f\"Please follow the instruction to check if the following content is product information or not and summarize the information into a JSON format with attributes (product_name, product_detail, and product_application) if it is a product information page: {prod}\"\n",
    "                    )\n",
    "                    if is_seller:\n",
    "                        with client.beta.threads.runs.stream(\n",
    "                            thread_id=thread.id,\n",
    "                            assistant_id=\"\",\n",
    "                            event_handler=EventHandler(),\n",
    "                        ) as stream:\n",
    "                            stream.until_done()\n",
    "                    else:\n",
    "                        with client.beta.threads.runs.stream(\n",
    "                            thread_id=thread.id,\n",
    "                            assistant_id=\"\",\n",
    "                            event_handler=EventHandler(),\n",
    "                        ) as stream:\n",
    "                            stream.until_done()\n",
    "                    # Retrieve the final message    \n",
    "                    messages = client.beta.threads.messages.list(\n",
    "                        thread_id=thread.id\n",
    "                    )\n",
    "                    final_message = messages.data[0].content[0].text.value\n",
    "                    queue.put(final_message)\n",
    "                    # Retrieve the final tokens\n",
    "                    runs = client.beta.threads.runs.list(\n",
    "                    thread_id=thread.id\n",
    "                )\n",
    "                    final_tokens.append({\"completion_tokens\": runs.data[0].usage.completion_tokens, \n",
    "                    \"prompt_tokens\": runs.data[0].usage.prompt_tokens, \n",
    "                    \"total_tokens\": runs.data[0].usage.total_tokens})\n",
    "                    return\n",
    "                \n",
    "            elif response.status_code == 429:\n",
    "                wait_time = backoff_factor * (2 ** i) + random.uniform(1, 3)\n",
    "                print(f\"Rate limit exceeded for {link}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the webpage {link}, status code: {response.status_code}\")\n",
    "                queue.put(json.dumps({\"product_name\": \"failed\"}))\n",
    "                return\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"RequestException for URL {link}: {e}\")\n",
    "            time.sleep(backoff_factor * (2 ** i) + random.uniform(1, 3))\n",
    "\n",
    "    print(f\"Failed to retrieve the webpage {link} after {retries} retries due to rate limiting.\")\n",
    "    queue.put(json.dumps({\"product_name\": \"failed\"}))\n",
    "\n",
    "# Number of workers (concurrent threads)\n",
    "num_workers = 5\n",
    "\n",
    "# collect_product_pages_multithreaded is function to collect product pages using multi-threading\n",
    "def collect_product_pages_multithreaded(product_urls, scrapeops_api_key, headers, client, num_workers, is_seller):\n",
    "    results = []\n",
    "    results_queue = Queue()\n",
    "    final_tokens = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers = num_workers) as executor:\n",
    "        future_to_url = {executor.submit(process_url, link, results_queue, scrapeops_api_key, headers, client, is_seller, final_tokens): link for link in product_urls}\n",
    "        total_counter = 0\n",
    "        for future in as_completed(future_to_url):\n",
    "            total_counter += 1\n",
    "            link = future_to_url[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred for URL {link}: {e}\")\n",
    "                results_queue.put(json.dumps({\"product_name\": \"Pass\"}))\n",
    "\n",
    "    print(f\"Total number of tasks completed: {total_counter}\")\n",
    "\n",
    "    while not results_queue.empty():\n",
    "        results.append(results_queue.get())\n",
    "    \n",
    "    return results, final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if the company is a seller, False if the company is a buyer\n",
    "is_seller = False\n",
    "\n",
    "# Call collect_product_pages_single function to collect product pages\n",
    "results, final_tokens = collect_product_pages_multithreaded(product_urls, scrapeops_api_key, headers, client, num_workers, is_seller)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_and_save_results function to process and save the results to file\n",
    "def process_and_save_results(results, product_urls, company_name, is_seller, final_tokens):\n",
    "    clean = []\n",
    "\n",
    "    def is_valid_json(s):\n",
    "        try:\n",
    "            json.loads(s)\n",
    "        except ValueError:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        if is_valid_json(results[i]):\n",
    "            data = json.loads(results[i])\n",
    "            data[\"product_link\"] = product_urls[i]\n",
    "            data[\"company_name\"] = company_name\n",
    "            \n",
    "            # Check if the index exists in final_tokens\n",
    "            if i < len(final_tokens):\n",
    "                data[\"completion_tokens\"] = final_tokens[i].get(\"completion_tokens\", 0)\n",
    "                data[\"prompt_tokens\"] = final_tokens[i].get(\"prompt_tokens\", 0)\n",
    "                data[\"total_tokens\"] = final_tokens[i].get(\"total_tokens\", 0)\n",
    "            else:\n",
    "                data[\"completion_tokens\"] = 0\n",
    "                data[\"prompt_tokens\"] = 0\n",
    "                data[\"total_tokens\"] = 0\n",
    "\n",
    "            clean.append(data)\n",
    "        else:\n",
    "            print(f\"Invalid JSON string at index {i}: {results[i]}\")\n",
    "    print(clean)\n",
    "    \n",
    "    if is_seller:\n",
    "        file_name = f\"Sellers/{company_name}.json\"\n",
    "        with open(file_name, \"w\") as d:\n",
    "            json.dump(clean, d)\n",
    "        print(f\"Products information saved to {file_name}\")\n",
    "    else:\n",
    "        file_name = f\"Buyers/{company_name}.json\"\n",
    "        with open(file_name, \"w\") as d:\n",
    "            json.dump(clean, d)\n",
    "        print(f\"Products information saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call process_and_save_results function\n",
    "process_and_save_results(results, product_urls, company_name, is_seller, final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check JSON File Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "if is_seller:\n",
    "    with open(f\"Sellers/{company_name}.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "else:    \n",
    "    with open(f\"Buyers/{company_name}.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "# Count the number of items\n",
    "item_count = len(data)\n",
    "print(f'The JSON file contains {item_count} items.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def count_items_with_ingredients(data):\n",
    "    \"\"\"\n",
    "    Recursively counts items and items with non-empty 'ingredients' list in the given data.\n",
    "    Returns a tuple (total_items, items_with_ingredients)\n",
    "    \"\"\"\n",
    "    total_items = 0\n",
    "    items_with_ingredients = 0\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            t_items, t_with_ingredients = count_items_with_ingredients(item)\n",
    "            total_items += t_items\n",
    "            items_with_ingredients += t_with_ingredients\n",
    "    elif isinstance(data, dict):\n",
    "        if 'ingredients' in data:\n",
    "            total_items += 1\n",
    "            if isinstance(data['ingredients'], list) and len(data['ingredients']) > 0:\n",
    "                items_with_ingredients += 1\n",
    "        for value in data.values():\n",
    "            t_items, t_with_ingredients = count_items_with_ingredients(value)\n",
    "            total_items += t_items\n",
    "            items_with_ingredients += t_with_ingredients\n",
    "    return total_items, items_with_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ''\n",
    "\n",
    "# Iterate over all JSON files in the specified folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                total_items, items_with_ingredients = count_items_with_ingredients(data)\n",
    "                if total_items > 0:\n",
    "                    percentage = (items_with_ingredients / total_items) * 100\n",
    "                    print(f\"{filename}: The percentage of items with non-empty ingredients list is {percentage:.2f}%\")\n",
    "                else:\n",
    "                    print(f\"{filename}: No items found.\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing file {filename}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaipy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
